{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (4.44.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (4.67.1)\n",
      "Requirement already satisfied: nlpaug in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.1.11)\n",
      "Requirement already satisfied: nltk in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.9.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.3.2)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.18.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.37.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (63.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers scikit-learn pandas numpy matplotlib seaborn tqdm nlpaug nltk datasets tf-keras accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          100 non-null    int64 \n",
      " 1   count               100 non-null    int64 \n",
      " 2   hate_speech         100 non-null    int64 \n",
      " 3   offensive_language  100 non-null    int64 \n",
      " 4   neither             100 non-null    int64 \n",
      " 5   class               100 non-null    int64 \n",
      " 6   tweet               100 non-null    object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 5.6+ KB\n",
      "None\n",
      "       Unnamed: 0       count  hate_speech  offensive_language     neither  \\\n",
      "count  100.000000  100.000000   100.000000          100.000000  100.000000   \n",
      "mean    49.640000    3.120000     0.210000            2.660000    0.250000   \n",
      "std     29.222421    0.728635     0.498381            0.976698    0.687184   \n",
      "min      0.000000    3.000000     0.000000            0.000000    0.000000   \n",
      "25%     24.750000    3.000000     0.000000            2.000000    0.000000   \n",
      "50%     49.500000    3.000000     0.000000            3.000000    0.000000   \n",
      "75%     74.250000    3.000000     0.000000            3.000000    0.000000   \n",
      "max    100.000000    9.000000     3.000000            7.000000    3.000000   \n",
      "\n",
      "            class  \n",
      "count  100.000000  \n",
      "mean     1.050000  \n",
      "std      0.297294  \n",
      "min      0.000000  \n",
      "25%      1.000000  \n",
      "50%      1.000000  \n",
      "75%      1.000000  \n",
      "max      2.000000  \n",
      "Unnamed: 0            0\n",
      "count                 0\n",
      "hate_speech           0\n",
      "offensive_language    0\n",
      "neither               0\n",
      "class                 0\n",
      "tweet                 0\n",
      "dtype: int64\n",
      "label\n",
      "0    98\n",
      "1     2\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    98\n",
      "1    98\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Inspection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import string\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '../../Data/Dataset_2.csv'\n",
    "df = pd.read_csv(data_path, encoding='latin1', delimiter=',', quotechar='\"')\n",
    "\n",
    "# Inspect the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "# Map classes to binary labels\n",
    "df['label'] = df['class'].apply(lambda x: 1 if x == 0 else 0)  # 1: Hate Speech, 0: Non-Hate Speech\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[['tweet', 'label']]\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions (@user)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['clean_tweet'] = df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.label == 0]  # Non-Hate Speech\n",
    "df_minority = df[df.label == 1]  # Hate Speech\n",
    "\n",
    "# Data Balancing\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # Sample with replacement\n",
    "                                 n_samples=len(df_majority),    # Match majority class\n",
    "                                 random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Verify balanced class distribution\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "# Tokenization\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize and encode data\n",
    "def encode_data(texts, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "# Encode training and testing data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_balanced['clean_tweet'], df_balanced['label'], test_size=0.2, random_state=42, stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "train_encodings = encode_data(train_texts, tokenizer)\n",
    "val_encodings = encode_data(val_texts, tokenizer)\n",
    "\n",
    "# Dataset Creation for BERT\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HateSpeechDataset(train_encodings, train_labels)\n",
    "val_dataset = HateSpeechDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 10%|█         | 10/100 [00:38<04:49,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11268638074398041, 'eval_runtime': 1.1951, 'eval_samples_per_second': 33.469, 'eval_steps_per_second': 2.51, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 20%|██        | 20/100 [01:13<04:30,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01480932254344225, 'eval_runtime': 1.1261, 'eval_samples_per_second': 35.52, 'eval_steps_per_second': 2.664, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|███       | 30/100 [01:46<03:34,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0034730557817965746, 'eval_runtime': 1.124, 'eval_samples_per_second': 35.588, 'eval_steps_per_second': 2.669, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|████      | 40/100 [02:19<03:05,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0016525499522686005, 'eval_runtime': 1.1832, 'eval_samples_per_second': 33.806, 'eval_steps_per_second': 2.535, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 50%|█████     | 50/100 [02:51<02:28,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.001128064701333642, 'eval_runtime': 1.1659, 'eval_samples_per_second': 34.307, 'eval_steps_per_second': 2.573, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 60%|██████    | 60/100 [03:24<01:59,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008961202693171799, 'eval_runtime': 1.2386, 'eval_samples_per_second': 32.294, 'eval_steps_per_second': 2.422, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 70%|███████   | 70/100 [03:56<01:28,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007861878839321434, 'eval_runtime': 1.1954, 'eval_samples_per_second': 33.461, 'eval_steps_per_second': 2.51, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 80%|████████  | 80/100 [04:28<00:59,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007242898573167622, 'eval_runtime': 1.1902, 'eval_samples_per_second': 33.609, 'eval_steps_per_second': 2.521, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 90%|█████████ | 90/100 [05:05<00:36,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006911147502250969, 'eval_runtime': 1.5685, 'eval_samples_per_second': 25.502, 'eval_steps_per_second': 1.913, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 100/100 [06:18<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006797086098231375, 'eval_runtime': 1.6065, 'eval_samples_per_second': 24.899, 'eval_steps_per_second': 1.867, 'epoch': 10.0}\n",
      "{'train_runtime': 378.5218, 'train_samples_per_second': 4.121, 'train_steps_per_second': 0.264, 'train_loss': 0.04100136280059814, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0009760856628417969, 'eval_runtime': 5.8284, 'eval_samples_per_second': 6.863, 'eval_steps_per_second': 0.515, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Non-Hate Speech       1.00      1.00      1.00        20\n",
      "    Hate Speech       1.00      1.00      1.00        20\n",
      "\n",
      "       accuracy                           1.00        40\n",
      "      macro avg       1.00      1.00      1.00        40\n",
      "   weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT Model Training\n",
    "# Initialize model\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, use_cache=False, gradient_checkpointing=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir='./results_bert',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True, \n",
    "    fp16_full_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_bert = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_bert.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(results_bert)\n",
    "\n",
    "# BERT Model Evaluation\n",
    "# Predict on validation set\n",
    "predictions_bert = trainer_bert.predict(val_dataset)\n",
    "pred_labels_bert = predictions_bert.predictions.argmax(-1)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(val_labels, pred_labels_bert, target_names=['Non-Hate Speech', 'Hate Speech']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + CNN Model Definition\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True)\n",
    "        self.conv1d = nn.Conv1d(lstm_hidden_dim, cnn_hidden_dim, kernel_size=3, padding=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(cnn_hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle both float and long inputs\n",
    "        if x.dtype == torch.float32:\n",
    "            x = x.long()\n",
    "        \n",
    "        # Regular forward pass\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Reshape for CNN\n",
    "        conv_in = lstm_out.transpose(1, 2)\n",
    "        conv_out = self.conv1d(conv_in)\n",
    "        \n",
    "        # Global max pooling\n",
    "        pooled = torch.max(conv_out, dim=2)[0]\n",
    "        \n",
    "        # Final classification\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(dropped)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Building and Text Sequencing\n",
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, max_vocab_size=10000):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()  # Simple whitespace-based tokenization\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(counter.most_common(max_vocab_size))}\n",
    "    vocab[' '] = 0  # Padding token\n",
    "    return vocab\n",
    "\n",
    "# Convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=100):\n",
    "    tokens = text.split()\n",
    "    sequence = [vocab.get(token, 0) for token in tokens]  # Use 0 for unknown tokens\n",
    "    sequence = sequence[:max_len]  # Truncate if longer than max_len \n",
    "    sequence += [0] * (max_len - len(sequence))  # Pad if shorter than max_len\n",
    "    return sequence\n",
    "\n",
    "# Build vocabulary from the dataset\n",
    "vocab = build_vocab(df_balanced['clean_tweet'], max_vocab_size=10000)\n",
    "\n",
    "# Convert all texts to sequences\n",
    "X_sequences = [text_to_sequence(text, vocab, max_len=100) for text in df_balanced['clean_tweet']]\n",
    "X_sequences = torch.tensor(X_sequences, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split data (convert labels to NumPy array)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_sequences, \n",
    "    df_balanced['label'].values,  # Convert to NumPy array\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "# Create datasets for LSTM + CNN\n",
    "class HateSpeechDatasetLSTM_CNN(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        # Convert labels to a numpy array if it's a pandas Series\n",
    "        self.labels = torch.tensor(labels.to_numpy() if isinstance(labels, pd.Series) else labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets for LSTM + CNN\n",
    "train_dataset_lstm_cnn = HateSpeechDatasetLSTM_CNN(X_train, y_train)\n",
    "val_dataset_lstm_cnn = HateSpeechDatasetLSTM_CNN(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5468898057937622\n",
      "Validation Loss: 0.3733650545279185, Accuracy: 1.0\n",
      "Epoch 2/10, Loss: 0.2108850184828043\n",
      "Validation Loss: 0.060985119392474495, Accuracy: 1.0\n",
      "Epoch 3/10, Loss: 0.021182815986685456\n",
      "Validation Loss: 0.003503371961414814, Accuracy: 1.0\n",
      "Epoch 4/10, Loss: 0.0009577138407621533\n",
      "Validation Loss: 0.0005716304876841605, Accuracy: 1.0\n",
      "Epoch 5/10, Loss: 0.00018391310150036588\n",
      "Validation Loss: 0.000246094382115795, Accuracy: 1.0\n",
      "Epoch 6/10, Loss: 9.0331923274789e-05\n",
      "Validation Loss: 0.00017013044271152467, Accuracy: 1.0\n",
      "Epoch 7/10, Loss: 6.602233697776683e-05\n",
      "Validation Loss: 0.00014498387827188708, Accuracy: 1.0\n",
      "Epoch 8/10, Loss: 5.61921480766614e-05\n",
      "Validation Loss: 0.00013390692765824497, Accuracy: 1.0\n",
      "Epoch 9/10, Loss: 5.420879570010584e-05\n",
      "Validation Loss: 0.0001285583627274415, Accuracy: 1.0\n",
      "Epoch 10/10, Loss: 4.906716603727546e-05\n",
      "Validation Loss: 0.00012418506715524322, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for LSTM + CNN\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "lstm_hidden_dim = 128\n",
    "cnn_hidden_dim = 128\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model for LSTM + CNN\n",
    "model_lstm_cnn = LSTM_CNN(vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# Define training loop for LSTM + CNN\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, labels = batch\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/total}')\n",
    "\n",
    "# Create DataLoader for LSTM + CNN\n",
    "train_loader = DataLoader(train_dataset_lstm_cnn, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_lstm_cnn, batch_size=16, shuffle=False)\n",
    "\n",
    "# Train the LSTM + CNN model\n",
    "train_model(model_lstm_cnn, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented texts: 980, Augmented labels: 980\n",
      "Epoch 1/10, Loss: 0.002300557940836401\n",
      "Validation Loss: 1.699403934859826e-05, Accuracy: 1.0\n",
      "Epoch 2/10, Loss: 1.1309421925959167e-05\n",
      "Validation Loss: 8.718175179941074e-06, Accuracy: 1.0\n",
      "Epoch 3/10, Loss: 7.449379115884875e-06\n",
      "Validation Loss: 6.47841126616792e-06, Accuracy: 1.0\n",
      "Epoch 4/10, Loss: 5.7099986641764715e-06\n",
      "Validation Loss: 5.26134607904775e-06, Accuracy: 1.0\n",
      "Epoch 5/10, Loss: 4.685210910793103e-06\n",
      "Validation Loss: 4.446768434718251e-06, Accuracy: 1.0\n",
      "Epoch 6/10, Loss: 3.909315938024951e-06\n",
      "Validation Loss: 3.816298211252919e-06, Accuracy: 1.0\n",
      "Epoch 7/10, Loss: 3.3507437116618273e-06\n",
      "Validation Loss: 3.3113258192922028e-06, Accuracy: 1.0\n",
      "Epoch 8/10, Loss: 2.8941619147791258e-06\n",
      "Validation Loss: 2.889797159847755e-06, Accuracy: 1.0\n",
      "Epoch 9/10, Loss: 2.57029895985428e-06\n",
      "Validation Loss: 2.5846604520059675e-06, Accuracy: 1.0\n",
      "Epoch 10/10, Loss: 2.254601613111521e-06\n",
      "Validation Loss: 2.2979014564346772e-06, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "\n",
    "aug = SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_text(text, augmenter, num_aug=5):\n",
    "    try:\n",
    "        augmented = augmenter.augment(text, n=num_aug)\n",
    "        return augmented if augmented else []\n",
    "    except Exception as e:\n",
    "        print(f\"Augmentation failed for text: {text}. Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Reset index to ensure contiguous integer indexing\n",
    "df_balanced = df_balanced.reset_index(drop=True)  # Critical fix\n",
    "\n",
    "for i, text in enumerate(df_balanced['clean_tweet']):\n",
    "    try:\n",
    "        augmented = augment_text(text, aug)\n",
    "        if len(augmented) > 0:\n",
    "            augmented_texts.extend(augmented)\n",
    "            augmented_labels.extend([df_balanced['label'].iloc[i]] * len(augmented))\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting text: {text}. Error: {e}\")\n",
    "\n",
    "# Verify lengths match\n",
    "print(f\"Augmented texts: {len(augmented_texts)}, Augmented labels: {len(augmented_labels)}\")\n",
    "\n",
    "# Create augmented DataFrame\n",
    "df_augmented = pd.DataFrame({'clean_tweet': augmented_texts, 'label': augmented_labels})\n",
    "\n",
    "# Combine and split data\n",
    "df_combined = pd.concat([df_balanced, df_augmented]).reset_index(drop=True)\n",
    "\n",
    "# Convert combined texts to sequences\n",
    "X_combined = [text_to_sequence(text, vocab, max_len=100) for text in df_combined['clean_tweet']]\n",
    "X_combined = torch.tensor(X_combined, dtype=torch.long)\n",
    "\n",
    "# Split data with error handling\n",
    "try:\n",
    "    X_train_combined, X_val_combined, y_train_combined, y_val_combined = train_test_split(\n",
    "        X_combined, df_combined['label'], test_size=0.2, random_state=42, stratify=df_combined['label']\n",
    "    )\n",
    "except ValueError:\n",
    "    # Fallback if stratify fails\n",
    "    X_train_combined, X_val_combined, y_train_combined, y_val_combined = train_test_split(\n",
    "        X_combined, df_combined['label'], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_combined = HateSpeechDatasetLSTM_CNN(X_train_combined, y_train_combined)\n",
    "val_dataset_combined = HateSpeechDatasetLSTM_CNN(X_val_combined, y_val_combined)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=16, shuffle=True)\n",
    "val_loader_combined = DataLoader(val_dataset_combined, batch_size=16, shuffle=False)\n",
    "\n",
    "# Train with augmented data\n",
    "train_model(model_lstm_cnn, criterion, optimizer, train_loader_combined, val_loader_combined, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented texts: 980, Augmented labels: 980\n",
      "Epoch 1/10, Loss: 0.21434751825438716\n",
      "Validation Loss: 0.01193646084672461, Accuracy: 0.9957627118644068\n",
      "Epoch 2/10, Loss: 0.004503156672137142\n",
      "Validation Loss: 0.0017961252519550423, Accuracy: 1.0\n",
      "Epoch 3/10, Loss: 0.0011189450382578614\n",
      "Validation Loss: 0.0008884680554425965, Accuracy: 1.0\n",
      "Epoch 4/10, Loss: 0.0005553946715026849\n",
      "Validation Loss: 0.0004826804336820108, Accuracy: 1.0\n",
      "Epoch 5/10, Loss: 0.0003329137062653406\n",
      "Validation Loss: 0.0003304954358706406, Accuracy: 1.0\n",
      "Epoch 6/10, Loss: 0.0002258159727994175\n",
      "Validation Loss: 0.00023949839160195552, Accuracy: 1.0\n",
      "Epoch 7/10, Loss: 0.0001592193924483643\n",
      "Validation Loss: 0.0001831476777927795, Accuracy: 1.0\n",
      "Epoch 8/10, Loss: 0.00011962741236703104\n",
      "Validation Loss: 0.00014103800955732974, Accuracy: 1.0\n",
      "Epoch 9/10, Loss: 9.309392093416398e-05\n",
      "Validation Loss: 0.00011454651915604094, Accuracy: 1.0\n",
      "Epoch 10/10, Loss: 7.40914558449176e-05\n",
      "Validation Loss: 9.437453033266744e-05, Accuracy: 1.0\n",
      "Epoch 1/10, Loss: 3.039046\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 2/10, Loss: 2.545605\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 3/10, Loss: 2.534389\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 4/10, Loss: 2.594805\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 5/10, Loss: 2.665567\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 6/10, Loss: 2.602309\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 7/10, Loss: 2.613816\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 8/10, Loss: 2.531056\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 9/10, Loss: 2.569899\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 10/10, Loss: 2.660319\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "import nltk\n",
    "import copy\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "\n",
    "aug = SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_text(text, augmenter, num_aug=5):\n",
    "    try:\n",
    "        augmented = augmenter.augment(text, n=num_aug)\n",
    "        return augmented if augmented else []\n",
    "    except Exception as e:\n",
    "        print(f\"Augmentation failed for text: {text}. Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Reset index to ensure contiguous integer indexing\n",
    "df_balanced = df_balanced.reset_index(drop=True)  # Critical fix\n",
    "\n",
    "for i, text in enumerate(df_balanced['clean_tweet']):\n",
    "    try:\n",
    "        augmented = augment_text(text, aug)\n",
    "        if len(augmented) > 0:\n",
    "            augmented_texts.extend(augmented)\n",
    "            augmented_labels.extend([df_balanced['label'].iloc[i]] * len(augmented))\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting text: {text}. Error: {e}\")\n",
    "\n",
    "# Verify lengths match\n",
    "print(f\"Augmented texts: {len(augmented_texts)}, Augmented labels: {len(augmented_labels)}\")\n",
    "\n",
    "# Create augmented DataFrame\n",
    "df_augmented = pd.DataFrame({'clean_tweet': augmented_texts, 'label': augmented_labels})\n",
    "\n",
    "# Combine and split data\n",
    "df_combined = pd.concat([df_balanced, df_augmented]).reset_index(drop=True)\n",
    "\n",
    "# Convert combined texts to sequences\n",
    "X_combined = [text_to_sequence(text, vocab, max_len=100) for text in df_combined['clean_tweet']]\n",
    "X_combined = torch.tensor(X_combined, dtype=torch.long)\n",
    "\n",
    "# Split data with error handling\n",
    "try:\n",
    "    X_train_combined, X_val_combined, y_train_combined, y_val_combined = train_test_split(\n",
    "        X_combined, df_combined['label'], test_size=0.2, random_state=42, stratify=df_combined['label']\n",
    "    )\n",
    "except ValueError:\n",
    "    # Fallback if stratify fails\n",
    "    X_train_combined, X_val_combined, y_train_combined, y_val_combined = train_test_split(\n",
    "        X_combined, df_combined['label'], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Create datasets for LSTM + CNN\n",
    "train_dataset_combined = HateSpeechDatasetLSTM_CNN(X_train_combined, y_train_combined)\n",
    "val_dataset_combined = HateSpeechDatasetLSTM_CNN(X_val_combined, y_val_combined)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=16, shuffle=True)\n",
    "val_loader_combined = DataLoader(val_dataset_combined, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define hyperparameters for LSTM + CNN\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "lstm_hidden_dim = 128\n",
    "cnn_hidden_dim = 128\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model for LSTM + CNN\n",
    "model_lstm_cnn = LSTM_CNN(vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# Define training loop for LSTM + CNN\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, labels = batch\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/total}')\n",
    "\n",
    "# Train with augmented data\n",
    "train_model(model_lstm_cnn, criterion, optimizer, train_loader_combined, val_loader_combined, num_epochs=10)\n",
    "\n",
    "# Adversarial Training\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def adversarial_train(model, criterion, optimizer, train_loader, val_loader, epsilon=0.01, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids, labels = batch\n",
    "            \n",
    "            # STEP 1: Regular forward pass and loss calculation\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # STEP 2: Detach everything to avoid backward graph issues\n",
    "            loss_value = loss.item()  # Get the scalar value\n",
    "            \n",
    "            # STEP 3: Create a copy of the model for adversarial example generation\n",
    "            adv_model = copy.deepcopy(model)\n",
    "            \n",
    "            # STEP 4: Regular backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # STEP 5: Generate adversarial examples using the copied model\n",
    "            # Apply perturbation to parameters of the copied model\n",
    "            for p_orig, p_adv in zip(model.parameters(), adv_model.parameters()):\n",
    "                if p_orig.grad is not None:\n",
    "                    # Apply perturbation to the adversarial model\n",
    "                    p_adv.data = p_orig.data + epsilon * torch.sign(p_orig.grad)\n",
    "            \n",
    "            # STEP 6: Forward pass with adversarial model\n",
    "            adv_outputs = adv_model(input_ids)\n",
    "            adv_loss = criterion(adv_outputs, labels)\n",
    "            adv_loss_value = adv_loss.item()  # Get the scalar value\n",
    "            \n",
    "            # STEP 7: Update the original model with combined loss\n",
    "            # We'll scale the gradients by (1 + adv_loss_value/loss_value)\n",
    "            # This mimics the effect of adding the adversarial loss\n",
    "            if loss_value > 0:\n",
    "                scale_factor = 1.0 + (adv_loss_value / loss_value)\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.data *= scale_factor\n",
    "            \n",
    "            # STEP 8: Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track total loss\n",
    "            epoch_loss += (loss_value + adv_loss_value)\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}')\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, labels = batch\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_avg_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f'Validation Loss: {val_avg_loss:.6f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Adversarial train LSTM + CNN\n",
    "# Adversarial train LSTM + CNN\n",
    "adversarial_train(model_lstm_cnn, criterion, optimizer, train_loader_combined, val_loader_combined, epsilon=0.01, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
